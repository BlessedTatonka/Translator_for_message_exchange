{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fastapi\n",
    "# !pip install uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m:     Will watch for changes in these directories: ['/media/boris/F/Translator_for_message_exchange/src']\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
      "\u001b[32mINFO\u001b[0m:     Started reloader process [\u001b[36m\u001b[1m48197\u001b[0m] using \u001b[36m\u001b[1mstatreload\u001b[0m\n",
      "[NeMo W 2022-02-04 10:21:12 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2022-02-04 10:21:12 nemo_logging:349] /home/boris/anaconda3/lib/python3.7/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n",
      "[NeMo W 2022-02-04 10:21:13 nmse_clustering:54] Using eigen decomposition from scipy, upgrade torch to 1.9 or higher for faster clustering\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2022-02-04 10:21:13 nemo_logging:349] /home/boris/anaconda3/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      '\"sox\" backend is being deprecated. '\n",
      "    \n",
      "[NeMo W 2022-02-04 10:21:13 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-02-04 10:21:17 experimental:28] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-02-04 10:21:17 experimental:28] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-02-04 10:21:17 experimental:28] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-02-04 10:21:17 __init__:23] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[nltk_data] Downloading package punkt to /home/boris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-02-04 10:21:17 experimental:28] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/boris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-02-04 10:21:17 experimental:28] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-02-04 10:21:17 experimental:28] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo I 2022-02-04 10:21:17 cloud:56] Found existing object /home/boris/.cache/torch/NeMo/NeMo_1.4.0/nmt_en_ru_transformer6x6/5ecb5abae99986a5bcd7ed79417b8317/nmt_en_ru_transformer6x6.nemo.\n",
      "[NeMo I 2022-02-04 10:21:17 cloud:62] Re-using file from: /home/boris/.cache/torch/NeMo/NeMo_1.4.0/nmt_en_ru_transformer6x6/5ecb5abae99986a5bcd7ed79417b8317/nmt_en_ru_transformer6x6.nemo\n",
      "[NeMo I 2022-02-04 10:21:17 common:702] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2022-02-04 10:21:27 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: /tmp/tmpeeczwbct/tokenizer.all.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2022-02-04 10:21:27 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: /tmp/tmpeeczwbct/tokenizer.all.32000.BPE.model with r2l: False.\n",
      "[NeMo W 2022-02-04 10:21:27 modelPT:131] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    src_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_zh/processed/batches.tokens.cwmt_news.septokenizer.16000.pkl\n",
      "    tgt_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_zh/processed/batches.tokens.cwmt_news.septokenizer.16000.pkl\n",
      "    tokens_in_batch: 16000\n",
      "    clean: true\n",
      "    max_seq_length: 512\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    num_workers: 8\n",
      "    load_from_cached_dataset: true\n",
      "    \n",
      "[NeMo W 2022-02-04 10:21:27 modelPT:138] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    src_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2013-en-ru.clean.tok.src\n",
      "    tgt_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2013-en-ru.clean.tok.ref\n",
      "    tokens_in_batch: 512\n",
      "    clean: false\n",
      "    max_seq_length: 512\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    num_workers: 8\n",
      "    \n",
      "[NeMo W 2022-02-04 10:21:27 modelPT:144] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    src_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2014-en-ru.clean.tok.src\n",
      "    tgt_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2014-en-ru.clean.tok.ref\n",
      "    tokens_in_batch: 512\n",
      "    clean: false\n",
      "    max_seq_length: 512\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    num_workers: 8\n",
      "    \n",
      "[NeMo W 2022-02-04 10:21:27 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo I 2022-02-04 10:21:36 save_restore_connector:143] Model MTEncDecModel was successfully restored from /home/boris/.cache/torch/NeMo/NeMo_1.4.0/nmt_en_ru_transformer6x6/5ecb5abae99986a5bcd7ed79417b8317/nmt_en_ru_transformer6x6.nemo.\n",
      "[NeMo I 2022-02-04 10:21:36 cloud:56] Found existing object /home/boris/.cache/torch/NeMo/NeMo_1.4.0/nmt_ru_en_transformer6x6/3db82426b17db1ae7cc7ae4ee3e3679b/nmt_ru_en_transformer6x6.nemo.\n",
      "[NeMo I 2022-02-04 10:21:36 cloud:62] Re-using file from: /home/boris/.cache/torch/NeMo/NeMo_1.4.0/nmt_ru_en_transformer6x6/3db82426b17db1ae7cc7ae4ee3e3679b/nmt_ru_en_transformer6x6.nemo\n",
      "[NeMo I 2022-02-04 10:21:36 common:702] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2022-02-04 10:21:47 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: /tmp/tmprlu6clcf/tokenizer.all.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2022-02-04 10:21:47 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: /tmp/tmprlu6clcf/tokenizer.all.32000.BPE.model with r2l: False.\n",
      "[NeMo W 2022-02-04 10:21:47 modelPT:131] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    src_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_zh/processed/batches.tokens.cmwt.septokenizer.16000.pkl\n",
      "    tgt_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_zh/processed/batches.tokens.cmwt.septokenizer.16000.pkl\n",
      "    tokens_in_batch: 16000\n",
      "    clean: true\n",
      "    max_seq_length: 512\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    num_workers: 8\n",
      "    load_from_cached_dataset: true\n",
      "    reverse_lang_direction: true\n",
      "    \n",
      "[NeMo W 2022-02-04 10:21:47 modelPT:138] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    src_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2013-ru-en.clean.tok.src\n",
      "    tgt_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2013-ru-en.clean.tok.ref\n",
      "    tokens_in_batch: 512\n",
      "    clean: false\n",
      "    max_seq_length: 512\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    num_workers: 8\n",
      "    \n",
      "[NeMo W 2022-02-04 10:21:47 modelPT:144] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    src_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2014-ru-en.clean.tok.src\n",
      "    tgt_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2014-ru-en.clean.tok.ref\n",
      "    tokens_in_batch: 512\n",
      "    clean: false\n",
      "    max_seq_length: 512\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    num_workers: 8\n",
      "    \n",
      "[NeMo W 2022-02-04 10:21:47 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-02-04 10:21:50 save_restore_connector:143] Model MTEncDecModel was successfully restored from /home/boris/.cache/torch/NeMo/NeMo_1.4.0/nmt_ru_en_transformer6x6/3db82426b17db1ae7cc7ae4ee3e3679b/nmt_ru_en_transformer6x6.nemo.\n",
      "[NeMo I 2022-02-04 10:21:50 cloud:56] Found existing object /home/boris/.cache/torch/NeMo/NeMo_1.4.0/tts_en_fastpitch/9651f9eb32324e98f965b98e94978217/tts_en_fastpitch.nemo.\n",
      "[NeMo I 2022-02-04 10:21:50 cloud:62] Re-using file from: /home/boris/.cache/torch/NeMo/NeMo_1.4.0/tts_en_fastpitch/9651f9eb32324e98f965b98e94978217/tts_en_fastpitch.nemo\n",
      "[NeMo I 2022-02-04 10:21:50 common:702] Instantiating model from pre-trained checkpoint\n",
      "[NeMo W 2022-02-04 10:21:51 modelPT:131] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /raid/LJSpeech/nvidia_ljspeech_train.json\n",
      "    max_duration: null\n",
      "    min_duration: 0.1\n",
      "    sample_rate: 22050\n",
      "    trim: false\n",
      "    parser: null\n",
      "    drop_last: true\n",
      "    shuffle: true\n",
      "    batch_size: 48\n",
      "    num_workers: 12\n",
      "    \n",
      "[NeMo W 2022-02-04 10:21:51 modelPT:138] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /raid/LJSpeech/nvidia_ljspeech_val.json\n",
      "    max_duration: null\n",
      "    min_duration: 0.1\n",
      "    sample_rate: 22050\n",
      "    trim: false\n",
      "    parser: null\n",
      "    drop_last: false\n",
      "    shuffle: false\n",
      "    batch_size: 48\n",
      "    num_workers: 8\n",
      "    \n",
      "[NeMo I 2022-02-04 10:21:51 features:262] PADDING: 1\n",
      "[NeMo I 2022-02-04 10:21:51 features:279] STFT using torch\n",
      "[NeMo I 2022-02-04 10:21:51 save_restore_connector:143] Model FastPitchModel was successfully restored from /home/boris/.cache/torch/NeMo/NeMo_1.4.0/tts_en_fastpitch/9651f9eb32324e98f965b98e94978217/tts_en_fastpitch.nemo.\n",
      "[NeMo I 2022-02-04 10:21:51 cloud:56] Found existing object /home/boris/.cache/torch/NeMo/NeMo_1.4.0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo.\n",
      "[NeMo I 2022-02-04 10:21:51 cloud:62] Re-using file from: /home/boris/.cache/torch/NeMo/NeMo_1.4.0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo\n",
      "[NeMo I 2022-02-04 10:21:51 common:702] Instantiating model from pre-trained checkpoint\n",
      "[NeMo W 2022-02-04 10:21:53 modelPT:131] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.datalayers.MelAudioDataset\n",
      "      manifest_filepath: /home/fkreuk/data/train_finetune.txt\n",
      "      min_duration: 0.75\n",
      "      n_segments: 8192\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 64\n",
      "      num_workers: 4\n",
      "    \n",
      "[NeMo W 2022-02-04 10:21:53 modelPT:138] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.datalayers.MelAudioDataset\n",
      "      manifest_filepath: /home/fkreuk/data/val_finetune.txt\n",
      "      min_duration: 3\n",
      "      n_segments: 66150\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 5\n",
      "      num_workers: 4\n",
      "    \n",
      "[NeMo W 2022-02-04 10:21:53 features:240] Using torch_stft is deprecated and will be removed in 1.1.0. Please set stft_conv and stft_exact_pad to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n",
      "[NeMo I 2022-02-04 10:21:53 features:262] PADDING: 0\n",
      "[NeMo I 2022-02-04 10:21:53 features:279] STFT using torch\n",
      "[NeMo W 2022-02-04 10:21:53 features:240] Using torch_stft is deprecated and will be removed in 1.1.0. Please set stft_conv and stft_exact_pad to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n",
      "[NeMo I 2022-02-04 10:21:53 features:262] PADDING: 0\n",
      "[NeMo I 2022-02-04 10:21:53 features:279] STFT using torch\n",
      "[NeMo I 2022-02-04 10:21:54 save_restore_connector:143] Model HifiGanModel was successfully restored from /home/boris/.cache/torch/NeMo/NeMo_1.4.0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo.\n",
      "[NeMo W 2022-02-04 10:21:59 modelPT:131] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.asr.data.audio_to_text.AudioToCharWithPriorAndPitchDataset\n",
      "      manifest_filepath: ../../datasets/RUSLAN/train_manifest.json\n",
      "      max_duration: null\n",
      "      min_duration: 0.1\n",
      "      int_values: false\n",
      "      normalize: true\n",
      "      sample_rate: 22050\n",
      "      trim: false\n",
      "      sup_data_path: ./priors_test\n",
      "      n_window_stride: 256\n",
      "      n_window_size: 1024\n",
      "      pitch_fmin: 52.06\n",
      "      pitch_fmax: 490.0\n",
      "      pitch_avg: 266.1947700760956\n",
      "      pitch_std: 102.32474957051491\n",
      "      vocab:\n",
      "        notation: phonemes\n",
      "        punct: true\n",
      "        spaces: true\n",
      "        stresses: true\n",
      "        add_blank_at: None\n",
      "        pad_with_space: true\n",
      "        chars: true\n",
      "        improved_version_g2p: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 1\n",
      "      num_workers: 12\n",
      "    manifest_filepath: ../../datasets/RUSLAN/train_manifest.json\n",
      "    batch_size: 4\n",
      "    num_workers: 4\n",
      "    \n",
      "[NeMo W 2022-02-04 10:21:59 modelPT:138] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.asr.data.audio_to_text.AudioToCharWithPriorAndPitchDataset\n",
      "      manifest_filepath: ../../datasets/RUSLAN/test_manifest.json\n",
      "      max_duration: null\n",
      "      min_duration: null\n",
      "      int_values: false\n",
      "      normalize: true\n",
      "      sample_rate: 22050\n",
      "      trim: false\n",
      "      sup_data_path: ./priors_test\n",
      "      n_window_stride: 256\n",
      "      n_window_size: 1024\n",
      "      pitch_fmin: 52.06\n",
      "      pitch_fmax: 490.0\n",
      "      pitch_avg: 266.1947700760956\n",
      "      pitch_std: 102.32474957051491\n",
      "      vocab:\n",
      "        notation: phonemes\n",
      "        punct: true\n",
      "        spaces: true\n",
      "        stresses: true\n",
      "        add_blank_at: None\n",
      "        pad_with_space: true\n",
      "        chars: true\n",
      "        improved_version_g2p: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 1\n",
      "      num_workers: 12\n",
      "    manifest_filepath: ../../datasets/RUSLAN/test_manifest.json\n",
      "    batch_size: 1\n",
      "    num_workers: 4\n",
      "    \n",
      "[NeMo E 2022-02-04 10:21:59 vocabs:324] Torch distributed needs to be initialized before you initialized <nemo.collections.asr.data.vocabs.Phonemes object at 0x7f8ea19db350>. This class is prone to data access race conditions. Now downloading corpora from global rank 0. If other ranks pass this before rank 0, errors might result.\n",
      "[NeMo I 2022-02-04 10:22:00 features:262] PADDING: 1\n",
      "[NeMo I 2022-02-04 10:22:00 features:279] STFT using torch\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m48199\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:45950 - \"\u001b[1mGET /openapi.json HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:45952 - \"\u001b[1mGET /openapi.json HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uvicorn main:app --reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
