{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fastapi\n",
    "# !pip install uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m:     Will watch for changes in these directories: ['/media/boris/F/Translator_for_message_exchange/src']\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
      "\u001b[32mINFO\u001b[0m:     Started reloader process [\u001b[36m\u001b[1m12375\u001b[0m] using \u001b[36m\u001b[1mstatreload\u001b[0m\n",
      "[NeMo W 2022-01-28 03:33:41 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2022-01-28 03:33:41 nemo_logging:349] /home/boris/anaconda3/lib/python3.7/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n",
      "[NeMo W 2022-01-28 03:33:41 nmse_clustering:54] Using eigen decomposition from scipy, upgrade torch to 1.9 or higher for faster clustering\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2022-01-28 03:33:42 nemo_logging:349] /home/boris/anaconda3/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      '\"sox\" backend is being deprecated. '\n",
      "    \n",
      "[NeMo W 2022-01-28 03:33:42 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "2022-01-28 03:33:44.091604: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "[NeMo W 2022-01-28 03:33:44 experimental:28] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-28 03:33:44 experimental:28] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-28 03:33:44 experimental:28] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-28 03:33:44 __init__:23] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[nltk_data] Downloading package punkt to /home/boris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-01-28 03:33:44 experimental:28] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/boris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-01-28 03:33:44 experimental:28] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-28 03:33:44 experimental:28] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo I 2022-01-28 03:33:45 cloud:56] Found existing object /home/boris/.cache/torch/NeMo/NeMo_1.4.0/nmt_en_ru_transformer6x6/5ecb5abae99986a5bcd7ed79417b8317/nmt_en_ru_transformer6x6.nemo.\n",
      "[NeMo I 2022-01-28 03:33:45 cloud:62] Re-using file from: /home/boris/.cache/torch/NeMo/NeMo_1.4.0/nmt_en_ru_transformer6x6/5ecb5abae99986a5bcd7ed79417b8317/nmt_en_ru_transformer6x6.nemo\n",
      "[NeMo I 2022-01-28 03:33:45 common:702] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2022-01-28 03:33:54 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: /tmp/tmpt75zaq3z/tokenizer.all.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2022-01-28 03:33:54 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: /tmp/tmpt75zaq3z/tokenizer.all.32000.BPE.model with r2l: False.\n",
      "[NeMo W 2022-01-28 03:33:54 modelPT:131] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    src_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_zh/processed/batches.tokens.cwmt_news.septokenizer.16000.pkl\n",
      "    tgt_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_zh/processed/batches.tokens.cwmt_news.septokenizer.16000.pkl\n",
      "    tokens_in_batch: 16000\n",
      "    clean: true\n",
      "    max_seq_length: 512\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    num_workers: 8\n",
      "    load_from_cached_dataset: true\n",
      "    \n",
      "[NeMo W 2022-01-28 03:33:54 modelPT:138] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    src_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2013-en-ru.clean.tok.src\n",
      "    tgt_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2013-en-ru.clean.tok.ref\n",
      "    tokens_in_batch: 512\n",
      "    clean: false\n",
      "    max_seq_length: 512\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    num_workers: 8\n",
      "    \n",
      "[NeMo W 2022-01-28 03:33:54 modelPT:144] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    src_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2014-en-ru.clean.tok.src\n",
      "    tgt_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2014-en-ru.clean.tok.ref\n",
      "    tokens_in_batch: 512\n",
      "    clean: false\n",
      "    max_seq_length: 512\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    num_workers: 8\n",
      "    \n",
      "[NeMo W 2022-01-28 03:33:54 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo I 2022-01-28 03:34:00 save_restore_connector:143] Model MTEncDecModel was successfully restored from /home/boris/.cache/torch/NeMo/NeMo_1.4.0/nmt_en_ru_transformer6x6/5ecb5abae99986a5bcd7ed79417b8317/nmt_en_ru_transformer6x6.nemo.\n",
      "[NeMo I 2022-01-28 03:34:01 cloud:56] Found existing object /home/boris/.cache/torch/NeMo/NeMo_1.4.0/nmt_ru_en_transformer6x6/3db82426b17db1ae7cc7ae4ee3e3679b/nmt_ru_en_transformer6x6.nemo.\n",
      "[NeMo I 2022-01-28 03:34:01 cloud:62] Re-using file from: /home/boris/.cache/torch/NeMo/NeMo_1.4.0/nmt_ru_en_transformer6x6/3db82426b17db1ae7cc7ae4ee3e3679b/nmt_ru_en_transformer6x6.nemo\n",
      "[NeMo I 2022-01-28 03:34:01 common:702] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2022-01-28 03:34:10 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: /tmp/tmpqx3wt28c/tokenizer.all.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2022-01-28 03:34:10 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: /tmp/tmpqx3wt28c/tokenizer.all.32000.BPE.model with r2l: False.\n",
      "[NeMo W 2022-01-28 03:34:10 modelPT:131] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    src_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_zh/processed/batches.tokens.cmwt.septokenizer.16000.pkl\n",
      "    tgt_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_zh/processed/batches.tokens.cmwt.septokenizer.16000.pkl\n",
      "    tokens_in_batch: 16000\n",
      "    clean: true\n",
      "    max_seq_length: 512\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    num_workers: 8\n",
      "    load_from_cached_dataset: true\n",
      "    reverse_lang_direction: true\n",
      "    \n",
      "[NeMo W 2022-01-28 03:34:10 modelPT:138] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    src_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2013-ru-en.clean.tok.src\n",
      "    tgt_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2013-ru-en.clean.tok.ref\n",
      "    tokens_in_batch: 512\n",
      "    clean: false\n",
      "    max_seq_length: 512\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    num_workers: 8\n",
      "    \n",
      "[NeMo W 2022-01-28 03:34:10 modelPT:144] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    src_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2014-ru-en.clean.tok.src\n",
      "    tgt_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2014-ru-en.clean.tok.ref\n",
      "    tokens_in_batch: 512\n",
      "    clean: false\n",
      "    max_seq_length: 512\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    num_workers: 8\n",
      "    \n",
      "[NeMo W 2022-01-28 03:34:10 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-28 03:34:13 save_restore_connector:143] Model MTEncDecModel was successfully restored from /home/boris/.cache/torch/NeMo/NeMo_1.4.0/nmt_ru_en_transformer6x6/3db82426b17db1ae7cc7ae4ee3e3679b/nmt_ru_en_transformer6x6.nemo.\n",
      "[NeMo I 2022-01-28 03:34:13 cloud:56] Found existing object /home/boris/.cache/torch/NeMo/NeMo_1.4.0/tts_en_fastpitch/9651f9eb32324e98f965b98e94978217/tts_en_fastpitch.nemo.\n",
      "[NeMo I 2022-01-28 03:34:13 cloud:62] Re-using file from: /home/boris/.cache/torch/NeMo/NeMo_1.4.0/tts_en_fastpitch/9651f9eb32324e98f965b98e94978217/tts_en_fastpitch.nemo\n",
      "[NeMo I 2022-01-28 03:34:13 common:702] Instantiating model from pre-trained checkpoint\n",
      "[NeMo W 2022-01-28 03:34:14 modelPT:131] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /raid/LJSpeech/nvidia_ljspeech_train.json\n",
      "    max_duration: null\n",
      "    min_duration: 0.1\n",
      "    sample_rate: 22050\n",
      "    trim: false\n",
      "    parser: null\n",
      "    drop_last: true\n",
      "    shuffle: true\n",
      "    batch_size: 48\n",
      "    num_workers: 12\n",
      "    \n",
      "[NeMo W 2022-01-28 03:34:14 modelPT:138] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /raid/LJSpeech/nvidia_ljspeech_val.json\n",
      "    max_duration: null\n",
      "    min_duration: 0.1\n",
      "    sample_rate: 22050\n",
      "    trim: false\n",
      "    parser: null\n",
      "    drop_last: false\n",
      "    shuffle: false\n",
      "    batch_size: 48\n",
      "    num_workers: 8\n",
      "    \n",
      "[NeMo I 2022-01-28 03:34:14 features:262] PADDING: 1\n",
      "[NeMo I 2022-01-28 03:34:14 features:279] STFT using torch\n",
      "[NeMo I 2022-01-28 03:34:15 save_restore_connector:143] Model FastPitchModel was successfully restored from /home/boris/.cache/torch/NeMo/NeMo_1.4.0/tts_en_fastpitch/9651f9eb32324e98f965b98e94978217/tts_en_fastpitch.nemo.\n",
      "[NeMo I 2022-01-28 03:34:15 cloud:56] Found existing object /home/boris/.cache/torch/NeMo/NeMo_1.4.0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo.\n",
      "[NeMo I 2022-01-28 03:34:15 cloud:62] Re-using file from: /home/boris/.cache/torch/NeMo/NeMo_1.4.0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo\n",
      "[NeMo I 2022-01-28 03:34:15 common:702] Instantiating model from pre-trained checkpoint\n",
      "[NeMo W 2022-01-28 03:34:16 modelPT:131] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.datalayers.MelAudioDataset\n",
      "      manifest_filepath: /home/fkreuk/data/train_finetune.txt\n",
      "      min_duration: 0.75\n",
      "      n_segments: 8192\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 64\n",
      "      num_workers: 4\n",
      "    \n",
      "[NeMo W 2022-01-28 03:34:16 modelPT:138] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.datalayers.MelAudioDataset\n",
      "      manifest_filepath: /home/fkreuk/data/val_finetune.txt\n",
      "      min_duration: 3\n",
      "      n_segments: 66150\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 5\n",
      "      num_workers: 4\n",
      "    \n",
      "[NeMo W 2022-01-28 03:34:16 features:240] Using torch_stft is deprecated and will be removed in 1.1.0. Please set stft_conv and stft_exact_pad to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n",
      "[NeMo I 2022-01-28 03:34:16 features:262] PADDING: 0\n",
      "[NeMo I 2022-01-28 03:34:16 features:279] STFT using torch\n",
      "[NeMo W 2022-01-28 03:34:16 features:240] Using torch_stft is deprecated and will be removed in 1.1.0. Please set stft_conv and stft_exact_pad to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n",
      "[NeMo I 2022-01-28 03:34:16 features:262] PADDING: 0\n",
      "[NeMo I 2022-01-28 03:34:16 features:279] STFT using torch\n",
      "[NeMo I 2022-01-28 03:34:17 save_restore_connector:143] Model HifiGanModel was successfully restored from /home/boris/.cache/torch/NeMo/NeMo_1.4.0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo.\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m12377\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "can you help me with my job quest\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:52996 - \"\u001b[1mGET /synthesize/can%20you%20help%20me%20with%20my%20job%20quest?src_lang=en HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "can you help me with my job quest\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:52998 - \"\u001b[1mGET /synthesize/can%20you%20help%20me%20with%20my%20job%20quest?src_lang=en HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "can you help me with my job?\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:53000 - \"\u001b[1mGET /synthesize/can%20you%20help%20me%20with%20my%20job%26quest?src_lang=en HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[33mWARNING\u001b[0m:  StatReload detected file change in 'main.py'. Reloading...\n",
      "\u001b[32mINFO\u001b[0m:     Shutting down\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
      "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
      "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m12377\u001b[0m]\n",
      "[NeMo W 2022-01-28 03:37:37 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2022-01-28 03:37:37 nemo_logging:349] /home/boris/anaconda3/lib/python3.7/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n",
      "[NeMo W 2022-01-28 03:37:37 nmse_clustering:54] Using eigen decomposition from scipy, upgrade torch to 1.9 or higher for faster clustering\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2022-01-28 03:37:37 nemo_logging:349] /home/boris/anaconda3/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      '\"sox\" backend is being deprecated. '\n",
      "    \n",
      "[NeMo W 2022-01-28 03:37:38 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "2022-01-28 03:37:39.890352: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "[NeMo W 2022-01-28 03:37:40 experimental:28] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-28 03:37:40 experimental:28] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-28 03:37:40 experimental:28] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-28 03:37:40 __init__:23] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[nltk_data] Downloading package punkt to /home/boris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-01-28 03:37:40 experimental:28] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/boris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-01-28 03:37:40 experimental:28] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-28 03:37:40 experimental:28] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-28 03:37:40 cloud:56] Found existing object /home/boris/.cache/torch/NeMo/NeMo_1.4.0/nmt_en_ru_transformer6x6/5ecb5abae99986a5bcd7ed79417b8317/nmt_en_ru_transformer6x6.nemo.\n",
      "[NeMo I 2022-01-28 03:37:40 cloud:62] Re-using file from: /home/boris/.cache/torch/NeMo/NeMo_1.4.0/nmt_en_ru_transformer6x6/5ecb5abae99986a5bcd7ed79417b8317/nmt_en_ru_transformer6x6.nemo\n",
      "[NeMo I 2022-01-28 03:37:40 common:702] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2022-01-28 03:37:50 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: /tmp/tmp8pcy8dr7/tokenizer.all.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2022-01-28 03:37:50 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: /tmp/tmp8pcy8dr7/tokenizer.all.32000.BPE.model with r2l: False.\n",
      "[NeMo W 2022-01-28 03:37:50 modelPT:131] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    src_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_zh/processed/batches.tokens.cwmt_news.septokenizer.16000.pkl\n",
      "    tgt_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_zh/processed/batches.tokens.cwmt_news.septokenizer.16000.pkl\n",
      "    tokens_in_batch: 16000\n",
      "    clean: true\n",
      "    max_seq_length: 512\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    num_workers: 8\n",
      "    load_from_cached_dataset: true\n",
      "    \n",
      "[NeMo W 2022-01-28 03:37:50 modelPT:138] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    src_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2013-en-ru.clean.tok.src\n",
      "    tgt_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2013-en-ru.clean.tok.ref\n",
      "    tokens_in_batch: 512\n",
      "    clean: false\n",
      "    max_seq_length: 512\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    num_workers: 8\n",
      "    \n",
      "[NeMo W 2022-01-28 03:37:50 modelPT:144] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    src_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2014-en-ru.clean.tok.src\n",
      "    tgt_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2014-en-ru.clean.tok.ref\n",
      "    tokens_in_batch: 512\n",
      "    clean: false\n",
      "    max_seq_length: 512\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    num_workers: 8\n",
      "    \n",
      "[NeMo W 2022-01-28 03:37:50 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo I 2022-01-28 03:37:55 save_restore_connector:143] Model MTEncDecModel was successfully restored from /home/boris/.cache/torch/NeMo/NeMo_1.4.0/nmt_en_ru_transformer6x6/5ecb5abae99986a5bcd7ed79417b8317/nmt_en_ru_transformer6x6.nemo.\n",
      "[NeMo I 2022-01-28 03:37:56 cloud:56] Found existing object /home/boris/.cache/torch/NeMo/NeMo_1.4.0/nmt_ru_en_transformer6x6/3db82426b17db1ae7cc7ae4ee3e3679b/nmt_ru_en_transformer6x6.nemo.\n",
      "[NeMo I 2022-01-28 03:37:56 cloud:62] Re-using file from: /home/boris/.cache/torch/NeMo/NeMo_1.4.0/nmt_ru_en_transformer6x6/3db82426b17db1ae7cc7ae4ee3e3679b/nmt_ru_en_transformer6x6.nemo\n",
      "[NeMo I 2022-01-28 03:37:56 common:702] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2022-01-28 03:38:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: /tmp/tmpf59ff6o8/tokenizer.all.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2022-01-28 03:38:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: /tmp/tmpf59ff6o8/tokenizer.all.32000.BPE.model with r2l: False.\n",
      "[NeMo W 2022-01-28 03:38:06 modelPT:131] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    src_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_zh/processed/batches.tokens.cmwt.septokenizer.16000.pkl\n",
      "    tgt_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_zh/processed/batches.tokens.cmwt.septokenizer.16000.pkl\n",
      "    tokens_in_batch: 16000\n",
      "    clean: true\n",
      "    max_seq_length: 512\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    num_workers: 8\n",
      "    load_from_cached_dataset: true\n",
      "    reverse_lang_direction: true\n",
      "    \n",
      "[NeMo W 2022-01-28 03:38:06 modelPT:138] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    src_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2013-ru-en.clean.tok.src\n",
      "    tgt_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2013-ru-en.clean.tok.ref\n",
      "    tokens_in_batch: 512\n",
      "    clean: false\n",
      "    max_seq_length: 512\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    num_workers: 8\n",
      "    \n",
      "[NeMo W 2022-01-28 03:38:06 modelPT:144] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    src_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2014-ru-en.clean.tok.src\n",
      "    tgt_file_name: /home/sandeepsub/Datasets/wmt/wmt20_en_ru/parallel/newstest2014-ru-en.clean.tok.ref\n",
      "    tokens_in_batch: 512\n",
      "    clean: false\n",
      "    max_seq_length: 512\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    num_workers: 8\n",
      "    \n",
      "[NeMo W 2022-01-28 03:38:06 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo I 2022-01-28 03:38:08 save_restore_connector:143] Model MTEncDecModel was successfully restored from /home/boris/.cache/torch/NeMo/NeMo_1.4.0/nmt_ru_en_transformer6x6/3db82426b17db1ae7cc7ae4ee3e3679b/nmt_ru_en_transformer6x6.nemo.\n",
      "[NeMo I 2022-01-28 03:38:10 cloud:56] Found existing object /home/boris/.cache/torch/NeMo/NeMo_1.4.0/tts_en_fastpitch/9651f9eb32324e98f965b98e94978217/tts_en_fastpitch.nemo.\n",
      "[NeMo I 2022-01-28 03:38:10 cloud:62] Re-using file from: /home/boris/.cache/torch/NeMo/NeMo_1.4.0/tts_en_fastpitch/9651f9eb32324e98f965b98e94978217/tts_en_fastpitch.nemo\n",
      "[NeMo I 2022-01-28 03:38:10 common:702] Instantiating model from pre-trained checkpoint\n",
      "[NeMo W 2022-01-28 03:38:11 modelPT:131] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /raid/LJSpeech/nvidia_ljspeech_train.json\n",
      "    max_duration: null\n",
      "    min_duration: 0.1\n",
      "    sample_rate: 22050\n",
      "    trim: false\n",
      "    parser: null\n",
      "    drop_last: true\n",
      "    shuffle: true\n",
      "    batch_size: 48\n",
      "    num_workers: 12\n",
      "    \n",
      "[NeMo W 2022-01-28 03:38:11 modelPT:138] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /raid/LJSpeech/nvidia_ljspeech_val.json\n",
      "    max_duration: null\n",
      "    min_duration: 0.1\n",
      "    sample_rate: 22050\n",
      "    trim: false\n",
      "    parser: null\n",
      "    drop_last: false\n",
      "    shuffle: false\n",
      "    batch_size: 48\n",
      "    num_workers: 8\n",
      "    \n",
      "[NeMo I 2022-01-28 03:38:11 features:262] PADDING: 1\n",
      "[NeMo I 2022-01-28 03:38:11 features:279] STFT using torch\n",
      "[NeMo I 2022-01-28 03:38:11 save_restore_connector:143] Model FastPitchModel was successfully restored from /home/boris/.cache/torch/NeMo/NeMo_1.4.0/tts_en_fastpitch/9651f9eb32324e98f965b98e94978217/tts_en_fastpitch.nemo.\n",
      "[NeMo I 2022-01-28 03:38:11 cloud:56] Found existing object /home/boris/.cache/torch/NeMo/NeMo_1.4.0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo.\n",
      "[NeMo I 2022-01-28 03:38:11 cloud:62] Re-using file from: /home/boris/.cache/torch/NeMo/NeMo_1.4.0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo\n",
      "[NeMo I 2022-01-28 03:38:11 common:702] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-01-28 03:38:13 modelPT:131] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.datalayers.MelAudioDataset\n",
      "      manifest_filepath: /home/fkreuk/data/train_finetune.txt\n",
      "      min_duration: 0.75\n",
      "      n_segments: 8192\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 64\n",
      "      num_workers: 4\n",
      "    \n",
      "[NeMo W 2022-01-28 03:38:13 modelPT:138] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.datalayers.MelAudioDataset\n",
      "      manifest_filepath: /home/fkreuk/data/val_finetune.txt\n",
      "      min_duration: 3\n",
      "      n_segments: 66150\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 5\n",
      "      num_workers: 4\n",
      "    \n",
      "[NeMo W 2022-01-28 03:38:13 features:240] Using torch_stft is deprecated and will be removed in 1.1.0. Please set stft_conv and stft_exact_pad to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n",
      "[NeMo I 2022-01-28 03:38:13 features:262] PADDING: 0\n",
      "[NeMo I 2022-01-28 03:38:13 features:279] STFT using torch\n",
      "[NeMo W 2022-01-28 03:38:13 features:240] Using torch_stft is deprecated and will be removed in 1.1.0. Please set stft_conv and stft_exact_pad to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n",
      "[NeMo I 2022-01-28 03:38:13 features:262] PADDING: 0\n",
      "[NeMo I 2022-01-28 03:38:13 features:279] STFT using torch\n",
      "[NeMo I 2022-01-28 03:38:13 save_restore_connector:143] Model HifiGanModel was successfully restored from /home/boris/.cache/torch/NeMo/NeMo_1.4.0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo.\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m12624\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:53002 - \"\u001b[1mGET /synthesize/hello?src_lang=en HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:53004 - \"\u001b[1mGET /synthesize/audio%20example?src_lang=en HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:53006 - \"\u001b[1mGET /synthesize/can%20you%20help%20me%20with%20my%20job%26quest?src_lang=en HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uvicorn main:app --reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
